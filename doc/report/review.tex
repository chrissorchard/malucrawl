\section{Literature Review}

Previous reports in the relevant areas were researched. In this section we are
going to introduce some background knowledges and research contributions
summarised from three reports written by authoritative institutions. We will
see the importance of trending keywords in malware distribution, and the
methods and systems to measure and counter them. Approaches for malicious
website detection and analysis are also included. 

\subsection{Search-engine optimization} 

Search engine is a key tool for
exploring resources on the Internet and becomes increasingly significant in
people's daily life. Many famous search engines index web pages are using PageRank
algorithm, which states that the rank of a web page depends on the quantity and
quality of incoming links, and the higher is the rank, the more likely that the
web page will apear in the top results of a search query. Although search
engine providers do not usually reveal their algorithms in calculating
PageRanks in order to reduce spammers who gaming with the system, many main
features are still well-known. For example, many webpages have their main title
stored in the URLs, therefore the URLs themself may represent a summary of the
page contents, which is a feature that can be utilised by website administrators 
to rise PageRank.
\paragraph{}
A report produced by University of Washington and Microsoft
Research Silicon Vally introduces a widely used search results poisoning
technique named Search Engine Optimization (SEO) as well as a system that
automatically detects them, called deSEO.\cite{deseo} They claim to be "the
first to present a systematic study of search-result poisoning attacks and how
to detect them". SEO is a process that uses varies approaches to affect a web
page such that it will apear in higher positions of a search engine's search
results, and for most of the cases, a rise of PageRank is achieved. This
process can be exploited to support illegal websites, such as malicious
websites, with some additional unethical techniques (which will be introduced
later) therefore achieves conspicuous boosts in PageRanks. This is called SEO
attacks. According to the paper, a typical SEO attack consists three servers,
which are compromised server, redirection server and exploit server. The
process of exploitation can be divided into five steps: 
\paragraph{}
\begin{enumerate} 
\item The victim submit a common request to a search engine.
\item The victim click on a search result and is navigated to a malicious web
page hosted on a compromised server.
\item The compromised server fowards the request to a redirection server.
\item The redirect server selects an exploit server then navigates the user
to it.  
\item Exploitation of the vitim's browser or scareware displayed.  
\end{enumerate}

To compromise a server, the attackers often target specific popular software
with known vunlerabilities but are still used by many servers. The example in
the report is a shopping websites management software called osCommerce, and
the attackers compromise the servers by uploading php scripts. Those scripts
are likely to have funtions that can navigate through the server's file system,
find data, and upload further files etc. When a server is compromised, SEO
pages can then be generated. Those pages contains a large number of links
connects to other SEO pages (probably on other compromised servers), as well as
contents that relate to the keywords this page's URL focuses on. Such dense
link structure and high relevant contents are the main means to elevate the
PageRank. The generated SEO pages may have clocking techniques, which means
they present different contents to search engine crawlers and victims. When the
page is visited by a search engine crawler, links to other SEO pages and
poisoned keywords are displayed, and when a victim accesses, the user's browser
is forwarded to a redirection server. 

\paragraph{}
Redirection servers are often formed
with several layers before reaching the exploit server, where they redirect
within the compromised server network. In the paper's case the domains of those
servers are very similar and according to their IPs it is determined that those
servers are hosted in physical groups. The Exploit servers in that example have
191 domains being hosted on only two IPs, and the webpage is very cautious with
incoming visitors. The page detects search engine crawlers as well as referrer,
and also blocks IPs from search engine companies.  

\paragraph{} 
The
researchers of the report crawled across the network of SEO links and finially
determined 5400 compromised domains. Each domain have an average links of
80,000 to other domains with 8,000 keywords corresponding to the same number of
SEO pages, which proves the use of dense link structure in rising PageRank.
They also provided an estimation of the number of victims affected by this
group of domains, which was produced from a dataset collected for two and a
half months. The result was that over 81,000 users were directed to exploit
pages.
\paragraph{} 
The detection of SEO attacks is not a trivial task, as
those pages usually trigger malicious actions only after strict requirements
are met, such as depending on the execution result of a JavaScript with the
input of a user's mouse action. Fortunately the report proved that study only
the URLs' structures is sufficient to complete the detections. 


\subsection{Trending-term exploitation}
Beside from how a SEO attack is technically implemented, we wonder what kind 
of websites apply SEO and how those SEO pages are found by victims. SEO is a 
process of improving PageRank, and before a webpage's PageRank is taken into 
account their content must match with the keywords of a search query. Many
well-known public websites and search engines provide trending data in real
time (such as Google Trends and Twitter Trends), and the easiest method to
determine what people are searching is to collect trending keywords/terms. With
trending terms the webpage's contents are then usually generated by sending
search queries with those terms, and combine contents in the URLs returned.
\paragraph{}
In this section we focus on a report researching ad-filled and malicious
websites.\cite{fashioncrime} It claims to be the "first large-scale measurement
and analysis of trending-term exploitation on the web", and as a result they
concluded the ways of trending terms application in spams among the web, with a
huge dataset collected through nine months containing over 60 million search
results. The report includes a wide research from trending terms collection to 
economics analysis, however here we only introduce topics relevant to our 
project. 
\subsubsection{Data collection and classification}
A trending terms dataset is categorised into trending set and control set. 
Trending set is formed by fast-changing popular terms, obtained from Google 
Hot Trends. These terms will become "cooled" in near future and are dropped 
from the dataset if so. Control set consists constantly popular search 
keywords, with the purpose to find out any "phenomena unique to the trending 
nature". The terms are then automatically fed to search-engines with optimised 
time interval. According to their experiments, 85\% of results are identical 
when comparing the results obtained from tests with 10 minutes and 4 hours 
interval, and finially 4 hours interval is applied to all their programs. Top 
results of each search query are stored in the dataset for classification, 
where in their case 32 results of Google and 100 results of Yahoo! are used.
\paragraph{}
In the report they organise websites into three categories, benign, 
malware-distributing and ad-filled, where ad-filled sites are also called Made 
for AdSense (MFA). For malware-distributing sites they simply check them with 
Google Safe Browsing API, in contrast for MFA sites they developed a 
supervised machine-learning algorithm then completed it with 363 manally 
inspected MFA sites as training set. As a result, for all URLs collected, 
0.30\% from the trending set and 0.13\% from the control set are determined to 
have malware and 6.2\% from the trending set are MFA sites. 
\subsubsection*{Measurement of trending-term abuse}
This report referred number of times to John et al.\cite{deseo}, which we 
reviewed in previous section. The authors confirmed dense link structures in 
both MFA and malware-serving websites. For MFA sites they built a directed 
graph and proved the existence of a "fairly large, collusive MFA operation" 
who owns 30.7\% of all MFA sites, and for malware-serving websites, as 
reported in the previous section, they found that a large portion of 
compromised servers are controlled by a single group, named a campaign. The 
same pattern appears in Twitter posts containing MFA links. Their data 
illustrates that 50\% of the MFA tweets direct to only 7.2\% of MFA domains 
across all MFA domains involved in Twitter. 
\paragraph{}
The authors then carried out a research in examining characteristics of 
trending terms. They classify trending keywords with Google Insights for 
Search, then find out the peak popularity estimate and advertising value 
provided by Traffic Estimator for each keyword. An observation is that all 
categories of keywords are targeted by malware and MFA, however the 
distribution of malicious sites varies according to a keyword's popularity and 
advertising value. More specifically, referring to their data, the percentages 
of malware-distribution and MFA sites decrease as the keyword's peak 
popularity and advertising value rises. They also conclude that, as a source 
of revenue, "malware and MFA behave more like substitutes, rather than 
complements".  
\subsubsection{Search-engine intervention}
Google stated some modifications of its search-engine ranking algorithm in 
response of increasing SEO behaviors on 24th Februrary, 2011. As the date was 
between their project's data collection duration, the report includes an 
investigation of the effectiveness about this announcement. \\
They compared the percentage of MFA sites in top 10 search results, with the 
conclusion states a decrease from 3.1\% to 0.47\% after a month later to the 
announcement date. They also collected the number of visits to MFA sites, 
which proved that the visits to MFA sites using Google ads had been decreased 
by 41\%, where those without had a reduction of 93\%. 

\subsection{Automatic malware collection}
We have introduced one of the most popular means to distribute malware (SEO) 
with trending terms as a significant role. In this section we will know the 
methods to detect malicious websites and analysis malwares in them, and 
finially collects malicious code across the web. A report carried out by Korea 
Internect \& Security Agency\cite{automalwarecollecting} explained a system 
that collects malicious code from malware-distributing websites found by 
search-engine with trending keywords. The report presents three approaches 
categorised into active and passive for detecting whether a website is 
malicious: 
\begin{enumerate}
\item 
{\bf Passive Malware Detection}\\
In this method we let attackers infest the system, then perform local analysis 
with the malware. Server honeypot is an example of this approach, which hosts 
a vunlerable system and waits for exploitation.   
\item
{\bf Active Low Interaction Client Honeypot}\\
In this case the malicious website is accessed, with only HTML downloaded 
however not executed. Static analysis is then applied to the file. The main 
advantage of this approach is simplicity. This kind of honeypots requires 
minimal resources to maintain and employ and does not give visible 
risks.\cite{honeypots}
However limitation exists. The most obvious point is this kind of systems are 
designed to detect known threats, because there is no way to find out the 
behaviors of a new type of attack without executing them. Another problem is 
that we are unable to check the contents of a page if it contains interative 
scripts which cannot be downloaded. For example, a block of script may require 
sources from other files, which we cannot examine further without running it. 
\item
{\bf Active High Interaction Client Honeypot}\\
The term high interaction means the malicious webpage is actually visited by a 
real browser under a real vunlerable system. 
The file system and registory are then scanned for modification. This approach 
requires a light-weight operating system which can be reset without 
significant time cost. An obvious advantage is that a thorough test can be 
performed to avoid any dynamic content cannot be found by static analysis, 
which means we are able to collect further amount of information includes the 
full behavior of an attack. Another point is that the high interaction system 
does not predict how the attack will behave, instead it gives an open 
environment which monitors all activities.\cite{honeypots}
However it is still not perfect, as some malicious code may require specific 
enviroment in order to execute, such as a certain version of browser and 
operation system combination. The system also requires significantly more 
resources than low interaction methods, including memory, disk space and 
computing power. 
\end{enumerate} 
The two active methods can be combined to overcome their shortages, which 
creates a new hybrid method. 
\paragraph{}
In the report they implemented an automated malware collecting system 
ultilises both low and high interaction approaches. The overall execution flow 
of their system is as follwing:
\begin{enumerate}
\item Trending keywords (or ``search keyword that reflects the social issue'' 
as the report said) are collected from various search engines. Though details 
in this step is not described in the report. 
\item Search queries are sent with those trending terms to a number of search 
engines. The top URLs returned are stored in a URL dataset. 
\item A URL component check is performed as a low interaction detecting 
method. For each URL they download and crawl the webpage to find out any 
suspecious behaviours. The checked criteria including the existence of zero 
size iFrame, the intensity of obfucated scripts, frequency of \% symbol and 
frequncy of particular function calls. They discovered that an 
obfuscated JavaScript in a malicious webpage usually contains significantly 
longer unique character streams, and similar patterns occur for \% symbol 
frequency and function calls. In the end they concluded a term called web page 
entropy which sums all the features described above, which can be used to 
classify whether a webpage is malicious or not. 
\item Webpages recongnized as malicious are sent to a high interaction client 
honeypot which automatically visits the page and collects malicious code 
distributed. They utilise a single virtual machine with the most vulnerable 
environment, however details about the environment are not included. They 
isolate newly created files and prevent their execution while also normalise 
any obfuscated scripts downloaded. 
\end{enumerate} 
\paragraph{}
In the performance verification section a comparison of 
their system and other existing active malware detection systems (HoneyMonkey, 
HoneyClient and MonkeySpider) shows that the new hybrid one has significant 
advantages in static analysis, false positive rate and system recovery speed. 
It is also the only system that is able to recover obfuscated script and 
pre-process URLs. 
\paragraph{}
Their data collection period spans for one and a half month, resulting 1287 
unique suspecious file collected. They test these files in two steps: first 
scan them with commercial anti-virus software, four vaccine programs were used 
in their case,  then for the remaining non-virus files they send them to 
behaviour analysis systems such as ThreatExport and Norman SandBox. As a result 
a total of 986 are determined to be malicious, where 119 of them are only 
discovered by behaviour analysis systems. 
The URLs are collected from search-engines, email spams and blacklists. 
Unfortunately the report does not include details about the source and 
quantity of their trending terms, and size of the URL dataset is also unknown. 
